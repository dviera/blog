[
  {
    "path": "posts/2024-06-27-calibration-curve/",
    "title": "Calibration Curve",
    "description": "Probabilities can be much more informative than labels.",
    "author": [
      {
        "name": "D.G.",
        "url": {}
      }
    ],
    "date": "2024-06-27",
    "categories": [],
    "contents": "\r\nThe model predicts that you do not have cancer vs. The model predicts you are 49% likely to have cancer. I would be worry with the latter rather than the former.\r\nWe compare the actual probability vs. predicted probability. To calculate those probabilities we first create bins between 0 and 1. For each bin we assign the predicted probabilities to each bin and calculate the mean of them. Once assigned, we know the true labels of those probabilities and we calculate the mean of them for each bin. This is our actual probabilities.\r\n\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom sklearn.linear_model import LogisticRegression\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.datasets import fetch_covtype\r\n\r\n# Load the dataset\r\ndata = fetch_covtype(download_if_missing=True)\r\nX = data.data\r\ny = (data.target == 1).astype(int)  # Make it a binary classification problem\r\n\r\n# Split the data into training and testing sets\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\r\n\r\n# Train a logistic regression model using sklearn\r\nmodel = LogisticRegression(max_iter=10000)\r\nmodel.fit(X_train, y_train)\r\nLogisticRegression(max_iter=10000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(max_iter=10000)\r\n\r\n# Predict probabilities\r\nprobabilities = model.predict_proba(X_test)[:, 1]\r\n\r\nCalculate the predicted probabilities in each bin:\r\n\r\nimport pandas as pd\r\n\r\nn_bins = 5\r\nbins = np.linspace(0, 1, n_bins + 1)\r\npredicted = pd.DataFrame({'pred': probabilities, 'bins_idx':np.digitize(probabilities, bins)}).groupby('bins_idx').agg('mean')\r\n\r\npredicted\r\n              pred\r\nbins_idx          \r\n1         0.077440\r\n2         0.299023\r\n3         0.501409\r\n4         0.691401\r\n5         0.868659\r\n\r\nCalculate the actual probabilities:\r\n\r\nactual = pd.DataFrame({'actual': y_test, 'bins_idx':np.digitize(probabilities, bins)}).groupby('bins_idx').agg('mean')\r\n\r\nactual\r\n            actual\r\nbins_idx          \r\n1         0.065506\r\n2         0.298689\r\n3         0.552583\r\n4         0.691578\r\n5         0.786157\r\n\r\nPlot the calibration curve and compare with the sklearn implementation:\r\n\r\nfrom sklearn.calibration import calibration_curve\r\n\r\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, probabilities, n_bins=n_bins)\r\n\r\nplt.figure(figsize=(8, 6))\r\nplt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\r\nplt.plot(mean_predicted_value, fraction_of_positives, \"o-\", label=\"sklearn\", c = \"#3255a4\")\r\nplt.plot(predicted['pred'], actual['actual'], \"s--\", label=\"from scratch\", c = \"#ff2d21\")\r\nplt.xlabel(\"Predicted probability\")\r\nplt.ylabel(\"Actual probability\")\r\nplt.title(\"Calibration curve\")\r\nplt.legend()\r\nplt.show()\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-06-27-calibration-curve/calibration-curve_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2024-06-27T21:44:35-04:00",
    "input_file": "calibration-curve.knit.md"
  },
  {
    "path": "posts/2024-06-15-single-layer-perceptron-for-discrete-choice-model/",
    "title": "Single Layer Perceptron for Discrete Choice Model",
    "description": "Using a single layer perceptron to calculate the coefficients of a choice model and the willingness to pay. Compare the results with the Multinomial Logit Model.",
    "author": [
      {
        "name": "D.G.",
        "url": {}
      }
    ],
    "date": "2024-06-15",
    "categories": [],
    "contents": "\r\nA single layer perceptron with one neuron that receives a 2-dimensional array as input. This array represents the alternatives in the rows with features as columns. Many questions can be presented to each respondent showing different product alternatives every time. They have to choose one of them. The output is the probabilities for each row (alternative).\r\nInput Layer:\r\nInput: A 2D array where rows represent different alternatives and columns represent features.\r\nShape: (n_alternatives, n_features)\r\n\r\nSingle Neuron:\r\nThis neuron will compute a weighted sum of the input features without a bias term.\r\nActivation Function: Softmax (to output probabilities for each alternative).\r\n\r\nOutput:\r\nProbabilities for each alternative.\r\nShape: (n_alternatives,)\r\n\r\nMathematical Representation\r\nInput Layer:\r\nInput: A 2D array \\(\\mathbf{X}\\) of shape \\((m, n)\\), where \\(m\\) is the number of alternatives and \\(n\\) is the number of features.\r\nExample Input: \\(\\mathbf{X} = \\begin{bmatrix} x_{11} & x_{12} & \\cdots & x_{1n} \\\\ x_{21} & x_{22} & \\cdots & x_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{m1} & x_{m2} & \\cdots & x_{mn} \\end{bmatrix}\\)\r\n\r\nWeights:\r\nWeight vector \\(\\mathbf{w}\\) of size \\(n \\times 1\\).\r\n\r\nSingle Neuron Computation:\r\nFor each alternative \\(i\\), the input \\(\\mathbf{x}_i\\) (a row of the matrix \\(\\mathbf{X}\\)) is used to compute the weighted sum:\r\n\\[ z_i = \\mathbf{x}_i \\mathbf{w}\\]\r\nHere, \\(\\mathbf{x}_i\\) is \\(1 \\times n\\) and \\(\\mathbf{w}\\) is \\(n \\times 1\\), resulting in \\(z_i\\) being a scalar.\r\n\r\nSoftmax Activation:\r\nThe scores \\(z_i\\) are then passed through the softmax function to get the probabilities:\r\n\\[ p_i = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}} \\]\r\n\r\nCode snippet\r\nSimulated data set from [@chapman2015r]. Full code here.\r\n\r\nclass MLPChoice(nn.Module):\r\n    \"\"\"Some Information about MLPChoice\"\"\"\r\n    def __init__(self, n_chosen, n_features):\r\n        super(MLPChoice, self).__init__()\r\n        self.n_chosen = n_chosen # choose only 1 alternative of the 3\r\n        self.n_features = n_features\r\n        \r\n        self.hidden = nn.Sequential(\r\n            nn.Linear(in_features=self.n_features, out_features=self.n_chosen, bias=False),\r\n        )\r\n\r\n    def forward(self, x):\r\n        logits = self.hidden(x) # dim batch x 3 x 1\r\n        logits = logits.squeeze() # dim batch x 3\r\n\r\n        return logits\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2024-06-16T23:10:53-04:00",
    "input_file": "single-layer-perceptron-for-discrete-choice-model.knit.md"
  },
  {
    "path": "posts/2024-06-13-linearization-techniques-in-optimization-multiplication-of-binary-variables/",
    "title": "Linearization Techniques in Optimization: Multiplication of Binary Variables",
    "description": "Linearization technique for the multiplication of binary variables [@asghari2022transformation].",
    "author": [
      {
        "name": "D.G.",
        "url": {}
      }
    ],
    "date": "2024-06-13",
    "categories": [],
    "contents": "\r\nConsider two binary variables \\(x_i\\) with \\(i \\in \\{1,...,m\\}\\) and \\(y_j\\)\r\nwith \\(j \\in \\{1,...,n\\}\\). To linearize the term \\(x_i·y_j\\), which results\r\nfrom multiplying the binary variables, we replace it with an additional\r\nbinary variable:\r\n\\[z_{ij} = x_i·y_j, \\forall i \\in \\{1,...,m\\}, \\forall j \\in \\{1,...,n\\}\\].\r\nThe model including the non-linear term can be linearized by adding some\r\nnew constraints as follows:\r\n\\[z_{i_j} ≤ x_i, ∀i ∈ \\{1,...,m\\}, ∀j ∈ {\\{1,...,n}\\}\\tag{1}\\]\r\n\\[z_{ij} ≤ y_j, ∀i ∈ \\{1,...,m\\}, ∀j ∈ {\\{1,...,n}\\}\\tag{2}\\]\r\n\\[z_{ij} ≥ x_i + y_j − 1, ∀i ∈ \\{1,...,m\\}, ∀j ∈ {\\{1,...,n}\\} \\tag{3}\\]\r\n\\[z_{ij} ∈ {0, 1}, ∀i ∈ \\{1,...,m\\}, ∀j ∈ {\\{1,...,n}\\}\\tag{4}\\]\r\nApplication: Quadratic Assignment Problem (Nahmias and Olsen 2015)\r\nThe problem is to assign machines to locations. It could be to assign other types of facilities to locations. This problem, unlike the simple assignment problem, is that where I assign one facility will have an impact on the others because there are interactions between facilities such as the number of materials handling trips and the cost of making those trips. We would like to put close facilities that have a lot of interactions.\r\nModel\r\n\\[\\begin{align*}\r\n\r\n&\\text{$n$ = number of machines;}\\\\\r\n\r\n&\\text{$d_{jr}$ = cost of making a single materials handling trip from location $j$ to location $r$;}\\\\\r\n\r\n&\\text{$f_{ik}$ = mean number of trips per time period from machine $i$ to machine $k$;}\\\\\r\n\r\n&x_{ij} =\r\n\\begin{cases}\r\n      1 & \\text{if machine $i$ is assigned to location $j$}\\\\\r\n      0 & \\text{otherwise}\r\n\\end{cases}  \r\n    \r\n\\end{align*}\\]\r\n\\[\\begin{align}\r\n\r\n\\sum_{i=1}^n \\sum_{j=1}^n \\sum_{k=1}^n \\sum_{r=1}^n f_{ik} · d_{jr} · x_{ij} · x_{kr}\r\n\r\n\\\\\r\n\r\n\\sum_{i=1}^n x_{ij} = 1,  \\forall j = 1,...,n\r\n\r\n\\\\\r\n\r\n\\sum_{j=1}^n x_{ij} = 1,  \\forall i = 1,...,n\r\n\r\n\\\\\r\nx_{ij} \\space\\space binary, \\forall i = 1,...,n \\space\\space and \\space\\space \\forall j = 1,...,n\r\n\r\n\\end{align}\\]\r\nImplementation in Julia\r\nExample 10.20 from (Tompkins et al. 2010).\r\n\r\n\r\nusing JuMP\r\nusing CPLEX\r\n# using GLPK\r\n\r\nflow = [0 5 2 0\r\n    0 0 2 3\r\n    3 4 0 0\r\n    0 0 5 0]\r\n\r\ndistance = [0 5 10 4\r\n    4 0 6 7\r\n    8 5 0 5\r\n    6 6 5 0]\r\n\r\nnbFac, nbLoc = size(flow)\r\n\r\nmodel = Model(CPLEX.Optimizer)\r\n# model = Model(GLPK.Optimizer)\r\n\r\n@variable(model, x[1:nbFac, 1:nbLoc], Bin)\r\n\r\n# New binary variable to linearize\r\n@variable(model, z[1:nbFac, 1:nbLoc, 1:nbFac, 1:nbLoc], Bin)\r\n\r\nfor j in 1:nbLoc\r\n    @constraint(model, sum(x[i, j] for i in 1:nbFac) == 1)\r\nend\r\n\r\nfor i in 1:nbFac\r\n    @constraint(model, sum(x[i, j] for j in 1:nbLoc) == 1)\r\nend\r\n\r\n# Added constraints to linearize\r\nfor i in 1:nbFac\r\n    for j in 1:nbLoc\r\n        for k in 1:nbFac\r\n            for r in 1:nbLoc\r\n                @constraint(model, z[i, j, k, r] <= x[i, j])\r\n                @constraint(model, z[i, j, k, r] <= x[k, r])\r\n                @constraint(model, z[i, j, k, r] >= x[i, j] + x[k, r] - 1)\r\n            end\r\n        end\r\n    end\r\nend\r\n\r\n@objective(model, Min, sum(flow[i, k] * distance[j, r] * z[i, j, k, r] for i = 1:nbFac for j = 1:nbLoc for k = 1:nbFac for r = 1:nbLoc))\r\n\r\nprint(model)\r\n\r\noptimize!(model)\r\n\r\nobjective_value(model)\r\n\r\nvalue.(x)\r\n\r\n\r\n\r\n\r\nAsghari, Mohammad, Amir M Fathollahi-Fard, SMJ Mirzapour Al-E-Hashem, and Maxim A Dulebenets. 2022. “Transformation and Linearization Techniques in Optimization: A State-of-the-Art Survey.” Mathematics 10 (2): 283.\r\n\r\n\r\nNahmias, Steven, and Tava Lennon Olsen. 2015. Production and Operations Analysis. Waveland Press.\r\n\r\n\r\nTompkins, James A, John A White, Yavuz A Bozer, and Jose Mario Azaña Tanchoco. 2010. Facilities Planning. John Wiley & Sons.\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2024-06-15T23:00:11-04:00",
    "input_file": {}
  }
]
