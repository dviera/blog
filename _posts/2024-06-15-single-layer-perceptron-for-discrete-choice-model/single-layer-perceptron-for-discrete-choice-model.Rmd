---
title: "Single Layer Perceptron for Discrete Choice Model"
description: |
  Using a single layer perceptron to calculate the coefficients of a choice model and the willingness to pay. Compare the results with the Multinomial Logit Model.
author: D.G.
date: 2024-06-15
output:
  distill::distill_article:
    self_contained: false
---

A single layer perceptron with one neuron that receives a 2-dimensional array as input. This array represents the alternatives in the rows with features as columns. Many questions can be presented to each respondent showing different product alternatives every time. They have to choose one of them. The output is the probabilities for each row (alternative).

1. **Input Layer**:
   - Input: A 2D array where rows represent different alternatives and columns represent features.
   - Shape: `(n_alternatives, n_features)`

2. **Single Neuron**:
   - This neuron will compute a weighted sum of the input features without a bias term.
   - Activation Function: Softmax (to output probabilities for each alternative).

3. **Output**:
   - Probabilities for each alternative.
   - Shape: `(n_alternatives,)`

### Mathematical Representation

1. **Input Layer**:
   - Input: A 2D array \(\mathbf{X}\) of shape \((m, n)\), where \(m\) is the number of alternatives and \(n\) is the number of features.
   - Example Input: \(\mathbf{X} = \begin{bmatrix} x_{11} & x_{12} & \cdots & x_{1n} \\ x_{21} & x_{22} & \cdots & x_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ x_{m1} & x_{m2} & \cdots & x_{mn} \end{bmatrix}\)

2. **Weights and Bias**:
   - Weight vector \(\mathbf{w}\) of size \(n \times 1\).

3. **Single Neuron Computation**:
   - For each alternative \(i\), the input \(\mathbf{x}_i\) (a row of the matrix \(\mathbf{X}\)) is used to compute the weighted sum plus bias:
     \[ z_i = \mathbf{x}_i \mathbf{w} + b \]
   - Here, \(\mathbf{x}_i\) is \(1 \times n\) and \(\mathbf{w}\) is \(n \times 1\), resulting in \(z_i\) being a scalar.

4. **Softmax Activation**:
   - The scores \(z_i\) are then passed through the softmax function to get the probabilities:
     \[ p_i = \frac{e^{z_i}}{\sum_{j} e^{z_j}} \]



### Code snippet

Simulated data set from [@chapman2015r]. Full code [here](https://github.com/dviera/marketing-analytics/blob/master/nn_discrete_choice_torch.ipynb).

```{python eval=FALSE}

class MLPChoice(nn.Module):
    """Some Information about MLPChoice"""
    def __init__(self, n_chosen, n_features):
        super(MLPChoice, self).__init__()
        self.n_chosen = n_chosen # choose only 1 alternative of the 3
        self.n_features = n_features
        
        self.hidden = nn.Sequential(
            nn.Linear(in_features=self.n_features, out_features=self.n_chosen, bias=False),
        )

    def forward(self, x):
        logits = self.hidden(x) # dim batch x 3 x 1
        logits = logits.squeeze() # dim batch x 3


        return logits
```


